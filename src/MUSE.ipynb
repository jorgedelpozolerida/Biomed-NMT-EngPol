{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the sentences: 0.6605645867223872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load pre-trained word embeddings for English and Polish\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float64)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "en_embeddings = load_embeddings('../muse/wiki.multi.en.vec')\n",
    "pl_embeddings = load_embeddings('../muse/wiki.multi.pl.vec')\n",
    "\n",
    "# Sample sentences in English and Polish\n",
    "sentence_en = \"The violin sang melancholic melodies, echoing the sorrowful tales of lost love and distant dreams.\"\n",
    "sentence_pl = \"Biblioteka tętniła ekscytacją, gdy studenci z zapałem odkrywali ogromną kolekcję wiedzy, dyskutując o pomysłach i odkryciach.\"\n",
    "\n",
    "# Tokenize sentences\n",
    "tokens_en = sentence_en.lower().split()\n",
    "tokens_pl = sentence_pl.lower().split()\n",
    "\n",
    "# Get word embeddings for each word in the sentences\n",
    "def sentence_embedding(tokens, embeddings):\n",
    "    word_embeddings = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if len(word_embeddings) > 0:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentence_embedding_en = sentence_embedding(tokens_en, en_embeddings)\n",
    "sentence_embedding_pl = sentence_embedding(tokens_pl, pl_embeddings)\n",
    "\n",
    "# Compute similarity between the sentence embeddings\n",
    "if sentence_embedding_en is not None and sentence_embedding_pl is not None:\n",
    "    similarity = np.dot(sentence_embedding_en, sentence_embedding_pl) / (np.linalg.norm(sentence_embedding_en) * np.linalg.norm(sentence_embedding_pl))\n",
    "    print(f\"Similarity between the sentences: {similarity}\")\n",
    "else:\n",
    "    print(\"No embeddings found for the sentence words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the sentences: 0.6605645867223872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load pre-trained word embeddings for English and Polish\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = dict()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float64)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "en_embeddings = load_embeddings('../muse/wiki.multi.en.vec')\n",
    "pl_embeddings = load_embeddings('../muse/wiki.multi.pl.vec')\n",
    "\n",
    "sentence_en = \"The violin sang melancholic melodies, echoing the sorrowful tales of lost love and distant dreams.\"\n",
    "sentence_pl = \"Biblioteka tętniła ekscytacją, gdy studenci z zapałem odkrywali ogromną kolekcję wiedzy, dyskutując o pomysłach i odkryciach.\"\n",
    "\n",
    "# Tokenize sentences\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Get word embeddings for each word in the sentences\n",
    "def sentence_embedding(tokens, embeddings):\n",
    "    word_embeddings = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if len(word_embeddings) > 0:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Compute similarity between the sentence embeddings\n",
    "def compute_similarity(sentence_embedding_en, sentence_embedding_pl):\n",
    "    similarity = np.dot(sentence_embedding_en, sentence_embedding_pl) / (np.linalg.norm(sentence_embedding_en) * np.linalg.norm(sentence_embedding_pl))\n",
    "    return similarity\n",
    "\n",
    "tokens_en = preprocess_sentence(sentence_en)\n",
    "tokens_pl = preprocess_sentence(sentence_pl)\n",
    "\n",
    "sentence_embedding_en = sentence_embedding(tokens_en, en_embeddings)\n",
    "sentence_embedding_pl = sentence_embedding(tokens_pl, pl_embeddings)\n",
    "\n",
    "#Output the results\n",
    "if sentence_embedding_en is not None and sentence_embedding_pl is not None:\n",
    "    similarity = compute_similarity(sentence_embedding_en, sentence_embedding_pl)\n",
    "    print(f\"Similarity between the sentences: {similarity}\")\n",
    "else:\n",
    "    print(\"No embeddings found for the sentence words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the sentences: 0.5801157206672021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = dict()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float64)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Your sentence data and functions\n",
    "en_embeddings = load_embeddings('../muse/wiki.multi.en.vec')\n",
    "pl_embeddings = load_embeddings('../muse/wiki.multi.pl.vec')\n",
    "\n",
    "sentence_en = \"The violin sang melancholic melodies, echoing the sorrowful tales of lost love and distant dreams.\"\n",
    "sentence_pl = \"Biblioteka tętniła ekscytacją, gdy studenci z zapałem odkrywali ogromną kolekcję wiedzy, dyskutując o pomysłach i odkryciach.\"\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    return tokens\n",
    "\n",
    "def compute_similarity(sentence_embedding_en, sentence_embedding_pl):\n",
    "    similarity = np.dot(sentence_embedding_en, sentence_embedding_pl) / (np.linalg.norm(sentence_embedding_en) * np.linalg.norm(sentence_embedding_pl))\n",
    "    return similarity\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_en = TfidfVectorizer()\n",
    "tfidf_pl = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer with the sentences\n",
    "tfidf_en.fit([sentence_en])\n",
    "tfidf_pl.fit([sentence_pl])\n",
    "\n",
    "# Transform sentences to get TF-IDF values for each word\n",
    "tfidf_matrix_en = tfidf_en.transform([sentence_en])\n",
    "tfidf_matrix_pl = tfidf_pl.transform([sentence_pl])\n",
    "\n",
    "# Map words to their TF-IDF values\n",
    "word_to_tfidf_en = dict(zip(tfidf_en.get_feature_names_out(), tfidf_matrix_en.toarray()[0]))\n",
    "word_to_tfidf_pl = dict(zip(tfidf_pl.get_feature_names_out(), tfidf_matrix_pl.toarray()[0]))\n",
    "\n",
    "def sentence_embedding(tokens, embeddings, word_to_tfidf):\n",
    "    weighted_word_embeddings = []\n",
    "    for word in tokens:\n",
    "        if word in embeddings and word in word_to_tfidf:\n",
    "            weighted_embedding = embeddings[word] * word_to_tfidf[word]\n",
    "            weighted_word_embeddings.append(weighted_embedding)\n",
    "    if weighted_word_embeddings:\n",
    "        return np.mean(weighted_word_embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "tokens_en = preprocess_sentence(sentence_en)\n",
    "tokens_pl = preprocess_sentence(sentence_pl)\n",
    "\n",
    "# Get sentence embeddings using TF-IDF weighted word embeddings\n",
    "sentence_embedding_en = sentence_embedding(tokens_en, en_embeddings, word_to_tfidf_en)\n",
    "sentence_embedding_pl = sentence_embedding(tokens_pl, pl_embeddings, word_to_tfidf_pl)\n",
    "\n",
    "if sentence_embedding_en is not None and sentence_embedding_pl is not None:\n",
    "    similarity = compute_similarity(sentence_embedding_en, sentence_embedding_pl)\n",
    "    print(f\"Similarity between the sentences: {similarity}\")\n",
    "else:\n",
    "    print(\"No embeddings found for the sentence words.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the sentences: 0.6605645418167114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load Word2Vec embeddings\n",
    "def load_word2vec_embeddings(file_path):\n",
    "    embeddings = KeyedVectors.load_word2vec_format(file_path, binary=False)\n",
    "    return embeddings\n",
    "\n",
    "# Function to load embeddings using the existing load_embeddings function\n",
    "def load_embeddings(file_path, is_word2vec=False):\n",
    "    if is_word2vec:\n",
    "        return load_word2vec_embeddings(file_path)\n",
    "    else:\n",
    "        embeddings = dict()\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.rstrip().split(' ')\n",
    "                word = values[0]\n",
    "                vector = np.array(values[1:], dtype=np.float64)\n",
    "                embeddings[word] = vector\n",
    "        return embeddings\n",
    "\n",
    "# Load Word2Vec embeddings for English and Polish\n",
    "en_embeddings = load_embeddings('../muse/wiki.multi.en.vec', is_word2vec=True)\n",
    "pl_embeddings = load_embeddings('../muse/wiki.multi.pl.vec', is_word2vec=True)\n",
    "\n",
    "# Your sentences\n",
    "sentence_en = \"The violin sang melancholic melodies, echoing the sorrowful tales of lost love and distant dreams.\"\n",
    "sentence_pl = \"Biblioteka tętniła ekscytacją, gdy studenci z zapałem odkrywali ogromną kolekcję wiedzy, dyskutując o pomysłach i odkryciach.\"\n",
    "\n",
    "# Function to preprocess the sentence\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate sentence embeddings using Word2Vec representations\n",
    "def sentence_embedding(tokens, embeddings):\n",
    "    word_embeddings = []\n",
    "    for word in tokens:\n",
    "        if word in embeddings:\n",
    "            word_embeddings.append(embeddings[word])\n",
    "    if word_embeddings:\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tokens_en = preprocess_sentence(sentence_en)\n",
    "tokens_pl = preprocess_sentence(sentence_pl)\n",
    "\n",
    "# Get sentence embeddings using Word2Vec word embeddings\n",
    "sentence_embedding_en = sentence_embedding(tokens_en, en_embeddings)\n",
    "sentence_embedding_pl = sentence_embedding(tokens_pl, pl_embeddings)\n",
    "\n",
    "if sentence_embedding_en is not None and sentence_embedding_pl is not None:\n",
    "    similarity = np.dot(sentence_embedding_en, sentence_embedding_pl) / (np.linalg.norm(sentence_embedding_en) * np.linalg.norm(sentence_embedding_pl))\n",
    "    print(f\"Similarity between the sentences: {similarity}\")\n",
    "else:\n",
    "    print(\"No embeddings found for the sentence words.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
